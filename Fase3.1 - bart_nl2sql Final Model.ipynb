{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","mount_file_id":"1CrlOBw0toISpJVVh6AAFWpG4YkIrS7QR","authorship_tag":"ABX9TyMwU1g2gTu4fi0H5H5nWUKC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"mwMihHwW2b3M"},"outputs":[],"source":["import os\n","import json\n","import torch\n","import random\n","from google.colab import drive\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments, EarlyStoppingCallback\n","from torch.optim.lr_scheduler import ReduceLROnPlateau"]},{"cell_type":"markdown","source":["Montar disco"],"metadata":{"id":"BoqfmjN_47jV"}},{"cell_type":"code","source":["drive.mount('/content/drive')\n","SPIDER_PATH = \"/content/drive/My Drive/spider\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9an1tGJf49Ml","executionInfo":{"status":"ok","timestamp":1739630167133,"user_tz":360,"elapsed":11276,"user":{"displayName":"Osvaldo Mora (Admin)","userId":"05857513400168345488"}},"outputId":"dfbacb51-1946-4a2e-9b93-07e9c5283a86"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["Cargar entrenamiento"],"metadata":{"id":"nDFdvfc65HU1"}},{"cell_type":"code","source":["def load_data(filepath):\n","    with open(filepath, \"r\") as f:\n","        return json.load(f)\n","\n","train_data = load_data(f\"{SPIDER_PATH}/train_spider.json\")\n","val_data = load_data(f\"{SPIDER_PATH}/dev.json\")"],"metadata":{"id":"N9oMCPYH5GKu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Pre procesar datos"],"metadata":{"id":"xEvBfh5I5Qft"}},{"cell_type":"code","source":["def preprocess_spider(data):\n","    inputs = []\n","    targets = []\n","    for item in data:\n","        question = item[\"question\"]\n","        sql_query = item[\"query\"]\n","        inputs.append(f\"Translate to SQL: {question}\")\n","        targets.append(sql_query)\n","    return inputs, targets\n","\n","train_inputs, train_targets = preprocess_spider(train_data)\n","val_inputs, val_targets = preprocess_spider(val_data)"],"metadata":{"id":"8lF5TNJC5Tgu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Tokenizar Datos"],"metadata":{"id":"59QOXmgH5eSt"}},{"cell_type":"code","source":["MODEL_NAME = \"SwastikM/bart-large-nl2sql\"\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","\n","def tokenize_data(inputs, targets, tokenizer, max_input_len=512, max_target_len=128):\n","    model_inputs = tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=max_input_len, return_tensors=\"pt\")\n","    labels = tokenizer(targets, padding=\"max_length\", truncation=True, max_length=max_target_len, return_tensors=\"pt\").input_ids\n","    return model_inputs, labels\n","\n","train_encodings, train_labels = tokenize_data(train_inputs, train_targets, tokenizer)\n","val_encodings, val_labels = tokenize_data(val_inputs, val_targets, tokenizer)"],"metadata":{"id":"C3D5Kj_L5gV-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dataset Personalizado"],"metadata":{"id":"xIF49eZl5kot"}},{"cell_type":"code","source":["class SpiderDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.encodings[\"input_ids\"])\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item[\"labels\"] = torch.tensor(self.labels[idx])\n","        return item\n","\n","train_dataset = SpiderDataset(train_encodings, train_labels)\n","val_dataset = SpiderDataset(val_encodings, val_labels)"],"metadata":{"id":"-DeFB9lY5riV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cargar Modelo"],"metadata":{"id":"2T-VuAYt5vV-"}},{"cell_type":"code","source":["model = BartForConditionalGeneration.from_pretrained(MODEL_NAME)"],"metadata":{"id":"nAz31fqE5wzv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Parametrizacion del entrenamiento"],"metadata":{"id":"5GiYgcEL56i0"}},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    output_dir=\"/content/drive/My Drive/bart_nl2sql_finetuned\",\n","    evaluation_strategy=\"epoch\",  # Evaluar al final de cada √©poca\n","    save_strategy=\"epoch\",  # Guardar checkpoints al final de cada √©poca\n","    learning_rate=1e-5,  # Ajustado para fine-tuning en NL2SQL\n","    per_device_train_batch_size=8,  # Ajuste din√°mico seg√∫n disponibilidad de GPU\n","    per_device_eval_batch_size=8,\n","    gradient_accumulation_steps=4,  # Para simular batch m√°s grandes en GPUs limitadas\n","    weight_decay=0.05,  # Prevenci√≥n de overfitting\n","    warmup_steps=500,  # Mejor ajuste de tasa de aprendizaje\n","    logging_dir=\"/content/drive/My Drive/logs\",\n","    logging_steps=50,  # Frecuencia de logging\n","    save_total_limit=2,  # Mantener solo los √∫ltimos dos checkpoints\n","    num_train_epochs=10,  # Incremento de √©pocas para mejor ajuste\n","    report_to=\"none\",  # Evitar reportes en servidores externos\n","    load_best_model_at_end=True,  # Cargar el mejor modelo al final\n","    metric_for_best_model=\"eval_loss\",  # Definir m√©trica de evaluaci√≥n\n","    greater_is_better=False,  # Menor p√©rdida es mejor\n","    save_on_each_node=True,\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ozoPWtS259yM","executionInfo":{"status":"ok","timestamp":1739601175163,"user_tz":360,"elapsed":6,"user":{"displayName":"Osvaldo Mora (Admin)","userId":"05857513400168345488"}},"outputId":"36ced249-a25c-4dcf-b49d-0259d9ae4488"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["Scheduler para reducir el learning rate"],"metadata":{"id":"cNtgkHN1Crj0"}},{"cell_type":"code","source":["scheduler = ReduceLROnPlateau(\n","    optimizer=torch.optim.AdamW(model.parameters(), lr=1e-5),\n","    mode='min',\n","    factor=0.5,  # Reducir lr a la mitad cuando no mejore\n","    patience=2,  # Esperar 2 √©pocas antes de reducir lr\n","    verbose=True\n",")"],"metadata":{"id":"azum1AG8CsI7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Early Stop"],"metadata":{"id":"5zvcX-XQB9w7"}},{"cell_type":"code","source":["# Entrenador con Early Stopping\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],  # Detener si no mejora en 3 √©pocas\n",")"],"metadata":{"id":"Wv6ClqcKB_hj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Entrenador"],"metadata":{"id":"sY1tWirh6EDe"}},{"cell_type":"markdown","source":["Aplicar Entrenamiento"],"metadata":{"id":"uIrBucnk6J5F"}},{"cell_type":"code","source":["trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":713},"id":"jdshpzMl6LzG","executionInfo":{"status":"ok","timestamp":1739603085185,"user_tz":360,"elapsed":1909640,"user":{"displayName":"Osvaldo Mora (Admin)","userId":"05857513400168345488"}},"outputId":"1ebe8e11-2be6-4227-b4de-01d3fb311bb4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-47-757293780ece>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","<ipython-input-47-757293780ece>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item[\"labels\"] = torch.tensor(self.labels[idx])\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1095' max='2180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1095/2180 31:48 < 31:34, 0.57 it/s, Epoch 5/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.153800</td>\n","      <td>0.595448</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.203300</td>\n","      <td>0.258317</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.095500</td>\n","      <td>0.278421</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.061900</td>\n","      <td>0.299231</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.048200</td>\n","      <td>0.310771</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:2758: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","<ipython-input-47-757293780ece>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","<ipython-input-47-757293780ece>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item[\"labels\"] = torch.tensor(self.labels[idx])\n","<ipython-input-47-757293780ece>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","<ipython-input-47-757293780ece>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item[\"labels\"] = torch.tensor(self.labels[idx])\n","<ipython-input-47-757293780ece>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","<ipython-input-47-757293780ece>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item[\"labels\"] = torch.tensor(self.labels[idx])\n","<ipython-input-47-757293780ece>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","<ipython-input-47-757293780ece>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item[\"labels\"] = torch.tensor(self.labels[idx])\n","There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=1095, training_loss=1.034376820255088, metrics={'train_runtime': 1910.553, 'train_samples_per_second': 36.639, 'train_steps_per_second': 1.141, 'total_flos': 3.792433053696e+16, 'train_loss': 1.034376820255088, 'epoch': 5.0})"]},"metadata":{},"execution_count":52}]},{"cell_type":"markdown","source":["Guardar Modelo"],"metadata":{"id":"PwIT5z0H6Zv-"}},{"cell_type":"code","source":["model.save_pretrained(\"/content/drive/My Drive/bart_nl2sql_finetuned/final_model\")\n","tokenizer.save_pretrained(\"/content/drive/My Drive/bart_nl2sql_finetuned/final_model\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H5-Zb3CF6b0t","executionInfo":{"status":"ok","timestamp":1739603090163,"user_tz":360,"elapsed":4981,"user":{"displayName":"Osvaldo Mora (Admin)","userId":"05857513400168345488"}},"outputId":"e5fd7e5f-5ac7-42d5-ff4a-0ddefc5edc44"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('/content/drive/My Drive/bart_nl2sql_finetuned/final_model/tokenizer_config.json',\n"," '/content/drive/My Drive/bart_nl2sql_finetuned/final_model/special_tokens_map.json',\n"," '/content/drive/My Drive/bart_nl2sql_finetuned/final_model/vocab.json',\n"," '/content/drive/My Drive/bart_nl2sql_finetuned/final_model/merges.txt',\n"," '/content/drive/My Drive/bart_nl2sql_finetuned/final_model/added_tokens.json',\n"," '/content/drive/My Drive/bart_nl2sql_finetuned/final_model/tokenizer.json')"]},"metadata":{},"execution_count":53}]},{"cell_type":"markdown","source":["Metricas de Evaluacion"],"metadata":{"id":"Z99IhkhK6dCs"}},{"cell_type":"code","source":["eval_results = trainer.evaluate()\n","print(\"Resultados de Evaluaci√≥n:\")\n","for key, value in eval_results.items():\n","    print(f\"{key}: {value}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":242},"id":"2BfxurDx6jwm","executionInfo":{"status":"ok","timestamp":1739603107424,"user_tz":360,"elapsed":17271,"user":{"displayName":"Osvaldo Mora (Admin)","userId":"05857513400168345488"}},"outputId":"e7b648dd-d77d-44ba-a0a5-33d3c5a53f7e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-47-757293780ece>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","<ipython-input-47-757293780ece>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item[\"labels\"] = torch.tensor(self.labels[idx])\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='130' max='130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [130/130 00:17]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Resultados de Evaluaci√≥n:\n","eval_loss: 0.2583167850971222\n","eval_runtime: 17.2539\n","eval_samples_per_second: 59.929\n","eval_steps_per_second: 7.535\n","epoch: 5.0\n"]}]},{"cell_type":"markdown","source":["Prueba de preguntas aleatorias"],"metadata":{"id":"73r7I9Ge6m72"}}]}